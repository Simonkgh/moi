<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Mine of Information - Hive container is running beyond physical memory limits</title>
    <link rel="stylesheet" type="text/css" href="/assets/css/coderay.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/stylesheet.css">
    <link type="application/atom+xml" title="Mine of Information" rel="alternate" href="/atom.xml"> 

    <meta name="generator" content="nanoc 4.12.15"> 
    <meta name="author" content="Simon Kitching"> 
  </head>
  <body>
    <section id="header">
      <span class='title'>The Mine of Information</span> <span class='desc'>(Nuggets of Programming and Linux)</span>
    </section>
    <div id="main">
      <section id='navpane'>
        <section>
  <ul id="navicons">
      <li class="nav">
      <a href="/" title="Home"><img src="/assets/images/Home.png"></a>
      <a href="/archives/" title="Archives"><img src="/assets/images/Calendar.png"></a>
      <a href="/site/welcome" title="E-Mail"><img src="/assets/images/Envelope.png"></a>
      <a href="/atom.xml" title="Subscribe Feed"><img src="/assets/images/RSS.png"></a>
      </li>
  </ul>
</section>

<section>
  <h1>About</h1>
  <ul id="about">
    <li>
      <a href="/site/welcome">Welcome</a>
    </li>
  </ul>
</section>

<section>
<h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2023/12/monorepos/">Monorepos and Polyrepos</a>
      </li>
    
      <li class="post">
        <a href="/2023/12/httpapis/">HTTP APIs, REST APIs, and Others</a>
      </li>
    
      <li class="post">
        <a href="/2023/09/biden/">Biden on Democracy</a>
      </li>
    
      <li class="post">
        <a href="/2023/09/tech-breadth/">Maintaining Technical Depth</a>
      </li>
    
      <li class="post">
        <a href="/2023/08/vpns/">The Uselessness of Consumer VPNs</a>
      </li>
    
      <li class="post">
        <a href="/2023/06/microservices/">Some Aspects of Implementing Microservices..</a>
      </li>
    
      <li class="post">
        <a href="/2023/06/dtest-evolution-scrum-monad/">DDD, Architecture patterns, and More..</a>
      </li>
    
      <li class="post">
        <a href="/2023/05/testing/">Should Unit Tests Verify Requirements Only?</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Categories</h1>
  <ul id="categories">
    
      <li class="catlink">
        <a href='/category/Architecture/'>Architecture</a>
      </li>
    
      <li class="catlink">
        <a href='/category/BigData/'>BigData</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Cloud/'>Cloud</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Cryptography/'>Cryptography</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Git/'>Git</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Infrastructure/'>Infrastructure</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Java/'>Java</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Links/'>Links</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Linux/'>Linux</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Network/'>Network</a>
      </li>
    
      <li class="catlink">
        <a href='/category/OSGi/'>OSGi</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Off-topic/'>Off-topic</a>
      </li>
    
      <li class="catlink">
        <a href='/category/OpenWRT/'>OpenWRT</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Programming/'>Programming</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Security/'>Security</a>
      </li>
    
      <li class="catlink">
        <a href='/category/Site/'>Site</a>
      </li>
    
  </ul>
</section>


      </section>
  
      <section id='content'>
        
  <div class='page'>
    <h1>Hive container is running beyond physical memory limits</h1>
    <aside>First published on: September 3, 2017</aside>
    
    <article>
    <p>Categories: <a href='/category/BigData/'>BigData</a></p>
      
<h1 id="overview">Overview</h1>

<p>Recently I used the Hive interactive commandline tool to run a simple query against a very large table. The query failed (after some time) with:</p>

<pre><code>Status: Failed------------------------------------------------------------------
Application application_012345_0123 failed 2 times due to AM Container for appattempt_0123455_0123_00002 exited with  exitCode: -104
For more detailed output, check the application tracking page: https://somehost:someport/cluster/app/application_012345_0123 Then click on links to logs of each attempt.
Diagnostics: Container [pid=12345,containerID=container_e12_012345_0123_02_000001] is running beyond physical memory limits. Current usage: 4.0 GB of 4 GB
 physical memory used; 5.8 GB of 8.4 GB virtual memory used. Killing container.
Dump of the process-tree for container_e12_012345_0123_02_000001 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 12345 67890 1111 2222 (bash) 0 0 112233444 697 /bin/bash -c /usr/lib/jvm/java-1.8.0/bin/java
          -Djava.io.tmpdir=/path/to/USER/appcache/application_012345_0123/container_e12_012345_0123_02_000001/tmp
          -server
          -Djava.net.preferIPv4Stack=true -Dhdp.version=2.5.3.0-37
          -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC
          -Xmx3686m
          -Dlog4j.configuratorClass=org.apache.tez.common.TezLog4jConfigurator -Dlog4j.configuration=tez-container-log4j.properties
          -Dyarn.app.container.log.dir=/path/to/log/application_012345_0123/container_e12_012345_0123_02_000001
          -Dtez.root.logger=INFO,CLA -Dsun.nio.ch.bugLevel='' org.apache.tez.dag.app.DAGAppMaster --session
          1&gt;/path/to/log/application_012345/container_e12_012345/stdout
          2&gt;/path/to/log/application_012345_0123/container_e12_012345_0123_02_000001/stderr  

Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Failing this attempt. Failing the application.
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Application application_012345_0123 failed 2 times
  due to AM Container for appattempt_012345_0123_000002 exited with  exitCode: -104
</code></pre>

<p>Unfortunately, an internet search found many pages referencing similar errors, but no clear answers; some people found options that solved the problem for them, but nobody seemed to know <em>why</em> those options worked. After some research, here are <em>my</em> conclusions - a mix of research and guesswork. Corrections are welcome!</p>

<p>This page applies specifically to Hive 1.3.1 using Tez on Yarn (Hortonworks Data Platform 2.5.3). The principles may be useful for similar environments.</p>

<h1 id="the-causes">The Causes</h1>

<p>As far as I can see, this kind of memory problem can occur in two places:</p>

<ul>
  <li>The Tez application master program (ie driver process which coordinates tasks)</li>
  <li>A Tez task (ie a process running logic to actually read and transform input)</li>
</ul>

<p>It can also be caused by incorrect default Tez settings for the cluster - in particular, sysadmins who explicitly include a “-Xmx” option in the default JVM args used to start Tez processes (see later), and set it too high relative to the default container size (resulting in the JVM growing beyond the allowed size for the container).</p>

<h2 id="application-master">Application Master</h2>

<p>The application master is the most common cause of this problem, triggered by a job which has a very large number of tasks. Each task needs to be computed and stored in memory, its status tracked, and its results stored; obviously lots of tasks requires lots of memory.</p>

<p>By default each input file and each HDFS block within each file is a <em>data partition</em>, and a task is created for each partition. The number of tasks therefore depends primarily upon:</p>

<ul>
  <li>The number of distinct input files</li>
  <li>The number of blocks in the input</li>
</ul>

<p>A complicated query may make things slightly worse, but the number of tasks following a shuffle is fixed by the “bucketing factor” rather than the number of partitions in the input - and therefore the input-partitions rather than the query complexity is the most significant factor for the number of tasks.</p>

<p>Resolving the “exceeds physical memory” problem therefore requires either:</p>

<ul>
  <li>Reducing the number of input files,</li>
  <li>Allocating multiple HDFS blocks to each task, or</li>
  <li>Increasing the amount of memory for the application master process</li>
</ul>

<p>When an unpartitioned table consists of many small files, then the Hive command <code>alter table {tablename} concatenate</code> will combine smaller files into larger ones. It doesn’t reduce the table to a single file; by default each output file is one HDFS block in size. For a partitioned table, the concatenate command must be of form <code>alter table {tablename} partition(field=val, field=val) concatenate</code>. Unfortunately the partitions must be specified explicitly - no expressions or wildcards allowed. When a table has many partitions, the current recommended solution is to write a script to perform the concatenation.</p>

<p>When a table consists of some very large (multi-block) files then it may help to adjust the <em>stride</em> setting so that a single task reads a range of data consisting of multiple blocks. I haven’t got an example here - please add one in the comments if you know how to do this.</p>

<p>The easiest solution is of course just to allocate more memory to the application master process - see later.</p>

<h2 id="tez-tasks">Tez Tasks</h2>

<p>There are reports on the internet of this problem occurring in the “task host” processes (rather than the application master) - presumably some complicated queries can consume significant amounts of memory within tasks.</p>

<p>Solutions are:</p>

<ul>
  <li>To simplify the query,</li>
  <li>To use <em>more</em> tasks (and therefore less data per task), or</li>
  <li>To allocate more memory to the Tez processes running tasks</li>
</ul>

<h1 id="memory-allocation-principles">Memory Allocation Principles</h1>

<p>At runtime, Tez uses a process model similar to Spark, and different from traditional MapReduce. A single container is started via Yarn to run the application master process. This then requests N additional containers and in each container a generic “task host” process is started (a JVM). Tasks are then executed by the application master by sending a message to a task host process which creates a thread and runs the task.</p>

<p>Each Yarn node-manager process has a “memory resource quota” set on startup via per-host config option <code>yarn.nodemanager.resource.memory-mb</code> (or possibly <code>yarn.scheduler.maximum-allocation-mb</code>; they seem to be the same). This isn’t enforced by the operating system in any way - the yarn node manager doesn’t “reserve” the memory, just uses the value as a guide to indicate whether it can reasonably accept new container allocation requests or not. The value should be the physical amount of memory on the node minus some margin for the operating system itself and any other processes that might run on the same host.</p>

<p>When a Yarn client application or application master requests a new container from Yarn, it includes a “container memory size”. The container will only be allocated on a yarn node which has at least this much “free space” in its quota, and the free space value on that node is then decremented by that amount. For simplicity, config option <code>yarn.scheduler.minimum-allocation-mb</code> sets the “base unit” for memory allocation; any container request whose request memory size is not a multiple of this value gets rounded up to the next multiple.</p>

<p>Note that a “container” is not necessarily anything to do with “linux containers” or “docker containers” - it is a Yarn “logical resource reservation”.</p>

<p>Eventually the “owner” of the container sends a command-line string to the node-manager to start a process within that container (usually <code>java -jar ...</code>). The Yarn resource manager does not enforce any memory resource limits on the started process; when running on a fairly modern Linux it could potentially use cgroups to do this enforcement, but it currently does not - and for other platforms it has no way to enforce such limits at all. Instead, when monitoring is enabled then the node-manager periodically queries the OS to see how much memory the created process is using, and kills it if it has exceeded the size of the container (ie originally requested size rounded up to multiple of base units of memory). This monitoring is not 100% reliable - if a started process forks then the node-manager can lose track of which processes belong to the container and cannot track/kill them. However this is not a problem when using MapReduce, Tez, or Spark - the processes that those frameworks generate don’t fork.</p>

<p>When the process started in the container is a JVM (and it always is for MapReduce/Tez/Spark) then the JVM itself enforces a memory limit - JVM commandline parameter <code>-Xmx</code> specifies how much memory may be used for the user heap. If a Java application within that JVM allocates memory, and there is no more space on the current heap, and the <code>-Xmx</code> limit has been reached then the JVM throws an OutOfMemoryError. An application can catch this, but normally does not - in which case the JVM terminates.</p>

<p>The JVM will use more than the <code>-Xmx</code> limit: it also needs space for the stack, class cache, and for the JVM’s implementation itself. In addition, in recent JVMs applications have the ability to allocate “off heap memory buffers” which are not restricted by the <code>-Xmx</code> limit.</p>

<p>One thing that initially confused me is that recommendations from the internet set the container-size config options (eg <code>mapreduce.map.memory.mb</code> for mapreduce) to a larger value than the <code>-Xmx</code> value. Hopefully the above description makes the reason clear: the container size enforced by the node-manager needs to be large enough to hold the entire JVM in memory, not just the Java heap (<code>-Xmx</code>).</p>

<p>All of the above applies to both “task hosts” and the application master process.</p>

<h1 id="tez-sessions">Tez Sessions</h1>

<p>One additional complication is that Tez supports “sessions”. When using an interactive tool such as the <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveCLI">hive cli</a>, <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients">beeline</a> or a web-based tool such as <a href="https://zeppelin.apache.org/">zeppelin</a> then they start a Tez session which starts and holds onto one or more containers which are reused for multiple queries. This allows interactive responses to be much quicker (no need to wait on each query for Yarn to allocate the necessary containers) but does mean that changing config settings within the interactive session does not affect the already-created containers - only new ones.</p>

<h1 id="solutions-to-the-memory-issues">Solutions to the Memory Issues</h1>

<p>When using Hive-cli with Tez, the size requested for containers in which “tez task host” processes are started is set by a Hive config option. This option limits the overall amount of memory that a yarn node-manager will allow such processes to allocate; as noted earlier the node-manager periodically checks the size of the process running in the container and kills it if it exceeds the container’s requested size.</p>

<p>As Tez tasks only use on-heap memory, the <code>-Xmx</code> option used to start these processes is also critical - increasing the container size will not help when the JVM itself cannot expand.</p>

<p>Below is a description of how to change the container size, and the JVM heap size.</p>

<h2 id="options-for-node-manager-monitoring">Options for node manager monitoring</h2>

<p>These options are not settable by users, but just for reference:</p>

<ul>
  <li>
<code>yarn.nodemanager.pmem-check-enabled</code> – enables periodic monitoring of the resident-set-size memory usage (physical memory) of the process running within a container (default:true)</li>
  <li>
<code>yarn.nodemanager.vmem-check-enabled</code>  – as above, but monitors virtual memory usage, ie includes swapped-out data (default: false)</li>
</ul>

<h2 id="options-for-container-size-control">Options for container size control</h2>

<p>Now comes the complicated part - there are various overlapping and very poorly documented options for setting the size of Tez containers.</p>

<p>According to some links, the following options control how Tez jobs started by Hive behave:</p>

<ul>
  <li>
<code>hive.tez.container.size</code> – value in megabytes</li>
  <li><code>hive.tez.java.opts</code></li>
</ul>

<p>and when these are not set, then under Hive the following are used as fallbacks:</p>

<ul>
  <li><code>mapreduce.map.memory.mb</code></li>
  <li><code>mapreduce.map.java.opts</code></li>
</ul>

<p>However other documentation indicates that the following options control how Tez jobs behave:</p>

<ul>
  <li>
<code>tez.task.resource.memory.mb</code> – requested memory for Yarn containers running “task host” processes</li>
  <li>
<code>tez.task.launch.cmd-opts</code> – args for JVM instance started in above containers</li>
  <li>
<code>tez.am.resource.memory.mb</code> – requested memory for the Yarn container running an application master process</li>
  <li>
<code>tez.am.launch.cmd-opts</code> – args for JVM instance started in above container</li>
  <li>
<code>tez.container.max.java.heap.fraction</code> (default: 0.8) - see below</li>
</ul>

<p>In my particular environment (using hive cli tool with Hortonworks Data Platform 2.5.3), the latter options (pure tez ones) were relevant. Setting these in the hive cli before starting the query allowed it to run. I am not sure what effect the <code>hive.tez.*</code> options have, or why they exist.</p>

<p>UPDATE: shortly after writing this article, I triggered <a href="/bigdata/hive-memory-parts">another kind of OutOfMemory error in Hive</a>. That time, modifying <code>hive.tez.container.size</code> <em>did</em> have an effect - it let the query run long enough to at least log the real cause of the problem. Clearly there is some interesting (and apparently undocumented) interaction between the <code>hive.tez.*</code> and <code>tez.*</code> config params.</p>

<p>What is elegant about <code>tez.*.launch.cmd-opts</code> is that when options <code>Xmx</code> and <code>Xms</code> are not specified, then Tez calculates them via <code>tez.container.max.java.heap.fraction * tez.*.resource.memory.mb</code>. This means that as user there is just one value that needs to be configured (the resource.memory.mb) rather than also needing to configure the java opts. Tez java code does not allocate off-heap-memory, so using a ratio to compute heap space is fairly reliable.</p>

<p>Under HDP (at least) there is a configuration-file <code>/etc/tez/conf/tez-site.xml</code> which sets defaults for the above. These values are visible when running <code>set -v</code> from within the hive cli.</p>

<p>MapReduce configuration options are split into map-specific and reduce-specific variants. Tez does not have pure map or reduce phases; “task host” containers can run either type of task.</p>

<p>Config option <code>tez.queue.name</code> can also be useful; it controls which Yarn workqueue a job is submitted to (rather than inheriting a default from the hive cli). According to one of the links in the references section, setting this value causes queries to be launched in new yarn containers rather than reusing containers belonging to the current Tez session.</p>

<h2 id="options-for-jvm-option-control">Options for JVM option control:</h2>

<p>For my specific case (simple query against very large table), the following allowed the query to complete:</p>

<pre><code>hive
  --hiveconf queue=myqueue
  --hiveconf tez.am.resource.memory.mb=16394
  --hiveconf tez.am.launch.cmd-opts=""
</code></pre>

<p>Overriding <code>tez.am.launch.cmd-opts</code> was necessary because the sysadmins for the cluster had incorrectly included an explicit <code>-Xmx=3686m</code> in the default value (in file <code>/etc/tez/conf/tez-site.xml</code>). Removing this value allowed Tez to automatically calculate it vla <code>tez.am.resource.memory.mb * tez.container.max.java.heap.fraction</code>.</p>

<p>I used <code>--hiveconf</code> rather than starting the hive cli and then using <code>set</code> because I was concerned about the job being run within containers belonging to the Tez session, and that being set up using defaults rather than the explicitly-set values. Maybe this would not have been a problem, but setting config before starting the hive cli seemed safer.</p>

<p>Just as a last note, the hive cli tool is supposedly deprecated in favour of the beeline tool. However at the current time (at least with HDP2.5.3) beeline is <em>far inferior</em> to hive, and rather buggy.</p>

<h1 id="last-notes">Last Notes</h1>

<p>As mentioned at the start, the content here is based on fragmentary information from the internet, experiments, and guesswork. If you have additional information or corrections, please let me know!</p>

<h1 id="references">References</h1>

<ul>
  <li>
    <p><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_installing_manually_book/content/ref-d677ca50-0a14-4d9e-9882-b764e689f6db.1.html">Hortonworks Docs: Data Platform: Non-Ambari Cluster Installation Guide</a> – official config docs</p>
  </li>
  <li>
    <p><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.1/bk_command-line-installation/content/ref-ffec9e6b-41f4-47de-b5cd-1403b4c4a7c8.1.html">Hortonworks Docs: Configuring Tez</a></p>
  </li>
  <li>
    <p><a href="http://www.hadoop360.datasciencecentral.com/blog/hadoop-yarn-explanation-and-container-memory-allocations?context=featured">Andrei Macson: Hadoop Yarn and container memory allocation</a> - article similar to this one</p>
  </li>
  <li>
    <p><a href="https://community.hortonworks.com/articles/56636/hive-understanding-concurrent-sessions-queue-alloc.html">Hortonworks Community: Hive Tez Sessions</a></p>
  </li>
  <li>
    <p><a href="https://community.hortonworks.com/articles/14309/demystify-tez-tuning-step-by-step.html">Hortonworks Community: Tez Tuning</a></p>
  </li>
</ul>

    </article>
  </div>
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'mineofinformation'; // mineofinformation (disqus site id)
      var disqus_pageid = '/bigdata/hive-cli-memory/'; // /relative/path/to/article/dir

      var disqus_config = function () {
        this.page.identifier = disqus_pageid;
        this.page.url = 'https://moi.vonos.net' + disqus_pageid;
      };
      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  



      </section>
    </div>
    <section id="footer">
      <p>Copyright &copy; 2025 - Simon Kitching</p>
    </section>
  </body>
</html>

